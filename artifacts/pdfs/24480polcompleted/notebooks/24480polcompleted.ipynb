{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animal 911 Calls Extraction from Rainforest Cafe Report\n",
        "\n",
        "This PDF is a service call report covering 911 incidents at the Rainforest Cafe in Niagara Falls, NY. We're hunting for animals! The data is formatted as a spreadsheet within the PDF, and challenges include varied column widths, borderless tables, and large swaths of missing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install natural-pdf\n",
        "!pip install natural-pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the PDF file\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "pdf_url = \"https://pub-4e99d31d19cb404d8d4f5f7efa51ef6e.r2.dev/pdfs/24480polcompleted/24480polcompleted.pdf\"\n",
        "pdf_name = \"24480polcompleted.pdf\"\n",
        "\n",
        "if not os.path.exists(pdf_name):\n",
        "    print(f\"Downloading {pdf_name}...\")\n",
        "    urllib.request.urlretrieve(pdf_url, pdf_name)\n",
        "    print(f\"Downloaded {pdf_name}\")\n",
        "else:\n",
        "    print(f\"{pdf_name} already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animal 911 Calls Extraction from Rainforest Cafe Report\n",
        "\n",
        "This PDF is a service call report covering 911 incidents at the Rainforest Cafe in Niagara Falls, NY. We're hunting for animals! The data is formatted as a spreadsheet within the PDF, and challenges include varied column widths, borderless tables, and large swaths of missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from natural_pdf import PDF\n",
        "\n",
        "pdf = PDF(\"24480polcompleted.pdf\")\n",
        "pdf.show(cols=3, limit=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting a subset of pages\n",
        "\n",
        "We only want the spreadsheet pages, which start on page 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = pdf.pages[4:]\n",
        "pages.show(cols=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Excluding extra text\n",
        "\n",
        "If we look at the last page we see \"2770 Records Found\" at the bottom of the table, which we do *not* want in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[-1].show(crop=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to **exclude it** so it doesn't show up in our table or confuse the table detector. But instead of matching it exactly, what if we end up doing this with different sets of documents? Maybe across years? It's easier to match with a **regex**, so instead of a specific number of records found we can look for `____ Records Found`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "  pages[-1]\n",
        "  .find_all('text:regex(\\\\d+ Records Found)')\n",
        "  .show(crop=100)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Anything we can find we can exclude.** Depending on what we expect our data to look like, we can exclude two different ways.\n",
        "\n",
        "/// tab | Exclude on the last page\n",
        "We know the \"XXX results\" will always be on the last page, so we can add a simple text selector match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[-1].add_exclusion('text:regex(\\\\d+ Records Found)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "///\n",
        "/// tab | Exclude on all possible pages\n",
        "If we aren't sure whether the record counts will be on other pages besides the last page, we can add it to the PDF. This will apply it to every single page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf.add_exclusion(\n",
        "  lambda page: page.find_all('text:regex(\\\\d+ Records Found)')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "///\n",
        "\n",
        "Record counts: excluded!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[-1].show(exclusions='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building our table\n",
        "\n",
        "Now we need to build our table. **Let's take a look at what the first page looks like again.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[0].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting the tables with guides\n",
        "\n",
        "We're going to use **guides** to outline the table with the following steps:\n",
        "\n",
        "- Drop vertical lines **between the column headers**, then re-use these boundaries on each page.\n",
        "- For horizontal rows, we'll say **find every place where text starts with NF-**, since each row starts with `NF-00051026-24`. That way even if there are multi-line rows we shouldn't have a problem.\n",
        "\n",
        "There are two ways to do this: re-useable guides with lambdas or just manually updating your guide in a for loop.\n",
        "\n",
        "> You could *probably* do a raw `.extract_table()` on each page and combine them, but using grids makes things a bit more specific and controlled.\n",
        ">\n",
        "> For example, if an entire column is empty on one page the \"normal\" extraction method won't understand that it's missing data. If you base your guides off of a full/complete page, though, it knows the empty area represents a column with missing data.\n",
        "\n",
        "/// tab | Re-useable guides\n",
        "\n",
        "We'll start by using **Guides** to draw our boundaries. While we might be able to separate rows based on whitespace, it's easier to base the borders on the content of the page:\n",
        "\n",
        "- Vertical boundaries go at the start of each of the column headers\n",
        "- Each row is located by text that starts with`NF-`\n",
        "\n",
        "By default the last column (Main Officer) would be ignored since it doesn't have an \"ending\" vertical bar. To fix that I'll add `outer=\"last\"` so the outer area *after* the last boundary counts as a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from natural_pdf.analyzers.guides import Guides\n",
        "\n",
        "guide = Guides(pages[0])\n",
        "columns = ['Number', 'Date Occurred', 'Time Occurred', 'Location', 'Call Type', 'Description', 'Disposition', 'Main Officer']\n",
        "guide.vertical.from_content(columns, outer=\"last\")\n",
        "guide.horizontal.from_content(\n",
        "  lambda p: p.find_all('text:starts-with(NF-)')\n",
        ")\n",
        "guide.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how we used a **lambda** in the approach above. This means we don't just want the `NF-` content on the first page, we want it for *any page the guide is applied to.*\n",
        "\n",
        "We then say, apply this guide to every single page! Since the headers are only on the first page, we use `header=\"first\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_result = guide.extract_table(pages, header=\"first\")\n",
        "df = table_result.to_df()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "///\n",
        "\n",
        "/// tab | Guides with loops\n",
        "We'll start by drawing boundaries at the start of each of the column headers. Since there isn't a boundary to the right of the last column, we'll say `outer=\"last\"` to have the outer area after the last boundary count as a column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from natural_pdf.analyzers.guides import Guides\n",
        "\n",
        "base = Guides(pages[0])\n",
        "columns = ['Number', 'Date Occurred', 'Time Occurred', 'Location', 'Call Type', 'Description', 'Disposition', 'Main Officer']\n",
        "base.vertical.from_content(columns, outer=\"last\")\n",
        "base.horizontal.from_content(pages[0].find_all('text:starts-with(NF-)'))\n",
        "base.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll then pull out the first table from the first page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_table = base.extract_table().to_df()\n",
        "first_table.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're going to go through each additional page, extracting the table, adding it to a list of pandas dataframes. At the end we'll then combine them all into one big dataframe.\n",
        "\n",
        "Note that the first page is the **only one with column headers**. We used a simple `.to_df()` before, but now we need to say `headers=columns` to manually set the headers of each dataframe. If we didn't do this pandas wouldn't be able to stack them all together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataframes = [first_table]\n",
        "\n",
        "for page in pages:\n",
        "    guides = Guides(page)\n",
        "    guides.vertical = base.vertical\n",
        "    guides.horizontal.from_content(page.find_all('text:starts-with(NF-)'))\n",
        "    single_df = guides.extract_table().to_df(header=columns)\n",
        "    dataframes.append(single_df)\n",
        "print(\"We made\", len(dataframes), \"dataframes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use `pd.concat` to combine them all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.concat(dataframes, ignore_index=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "///\n",
        "\n",
        "Done!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}